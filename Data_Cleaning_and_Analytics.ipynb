{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of Packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import pos_tag, map_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "pstem = PorterStemmer()\n",
    "lem = WordNetLemmatizer()\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score\n",
    "\n",
    "#Downloading packages\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "!pip install wordcloud\n",
    "!pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting list of csv files contains extracted tweets.\n",
    "thisdir = os.getcwd()\n",
    "currdir = os.chdir('D:/Study/Dissertation/Practical/Test')\n",
    "dataset_path = os.listdir(currdir)\n",
    "dataset_path\n",
    "\n",
    "#Creating data_frame\n",
    "covidtweet_df = pd.DataFrame()\n",
    "\n",
    "#Loading tweets from csv into dataframe.\n",
    "for excel_files in dataset_path:\n",
    "    sheet = pd.read_csv(excel_files,encoding='utf-8')\n",
    "    covidtweet_df = covidtweet_df.append(sheet)\n",
    "    date = excel_files.split(\"_\")[0] #Extracting date from excel filename and adding it into new column.\n",
    "    covidtweet_df['Date'] = pd.to_datetime(date, format='%Y%m%d')  \n",
    "\n",
    "#Getting struture of the dataframe.\n",
    "covidtweet_df.info()\n",
    "\n",
    "#viewing top 5 rows from the dataframe.\n",
    "covidtweet_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for duplicates\n",
    "duplicates = covidtweet_df.duplicated().tolist()\n",
    "print(duplicates.count(True))\n",
    "duplicate_tweets = covidtweet_df.duplicated(['text']).tolist()\n",
    "print(duplicate_tweets.count(True))\n",
    "\n",
    "#Dropping duplicates.\n",
    "covidtweet_df = covidtweet_df.drop_duplicates(['text'])\n",
    "covidtweet_df.shape[0]\n",
    "\n",
    "# Checking null values & dropping it from location field\n",
    "print(covidtweet_df['location'].isna().sum())\n",
    "\n",
    "covidtweet_df = covidtweet_df.dropna(axis=0, subset=['location'])\n",
    "\n",
    "#Dropping unwanted columns from dataframe.\n",
    "covidtweet_df = covidtweet_df.drop(['hashtags','user','acctdesc','totaltweets','retweetcount'], axis = 1)\n",
    "covidtweet_df.head(5)\n",
    "\n",
    "#Renaming the column field for better naming conversion\n",
    "covidtweet_df.rename(columns = {'Unnamed: 0':'S.NO','text':'Tweets','location':'Location'}, inplace = True)\n",
    "print(covidtweet_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing county excel files to select specific countries.\n",
    "county_excel = pd.read_csv('D:\\Study\\Dissertation\\Practical\\Counties_list\\Counties.csv')\n",
    "county_df = pd.DataFrame(county_excel)\n",
    "county_df\n",
    "covidtweet_df.Location=covidtweet_df.Location.astype(str)\n",
    "county_df = county_df.astype(str)\n",
    "county_df.head(4)\n",
    "\n",
    "# Matching location field with county dataset for six countries such as USA, UK, Ireland, Australia, Italy and New Zealand.\n",
    "for (columnName, columnData) in county_df.iteritems():\n",
    "    for item in columnData:\n",
    "        covidtweet_df.loc[covidtweet_df['Location'].str.contains(item, na=False, case=False, regex=True), 'Country'] = columnName\n",
    "\n",
    "#Display bottom 5 rows of the dataframe\n",
    "covidtweet_df.tail(5)\n",
    "\n",
    "#Checking null values in country field\n",
    "print(covidtweet_df['Country'].isna().sum())\n",
    "\n",
    "# Dropping null values from Country field and dropping Location field\n",
    "covidtweet_df = covidtweet_df.dropna(axis=0, subset=['Country'])\n",
    "covidtweet_df = covidtweet_df.drop(['Location'], axis=1)\n",
    "\n",
    "# Display bottom 5 rows after removing duplicates from country field\n",
    "covidtweet_df.tail(5)\n",
    "\n",
    "#One batch were completed and exporting the dataframe into separate csv files.\n",
    "filename = 'coronavirus_tweets_sample_4.csv'\n",
    "covidtweet_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the filter tweets csv files for six countries into single final csv file.\n",
    "os.chdir('D:\\Study\\Dissertation\\Practical\\Merge_dataset')\n",
    "file_endswith = 'csv'\n",
    "Files = [x for x in glob.glob('*.{}'.format(file_endswith))]\n",
    "\n",
    "Concat_all_csv_files = pd.concat([pd.read_csv(file) for file in Files])\n",
    "final_covidtweet_df = pd.DataFrame()\n",
    "final_covidtweet_df = final_covidtweet_df.append(Concat_all_csv_files)\n",
    "\n",
    "#Displaying the top 5 rows.\n",
    "final_covidtweet_df.head(5)\n",
    "\n",
    "#Getting length of the dataframe\n",
    "final_covidtweet_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates in the row.\n",
    "final_covidtweet_df.shape[0]\n",
    "duplicates = final_covidtweet_df.duplicated().tolist()\n",
    "print(duplicates.count(True))\n",
    "duplicate_tweets = final_covidtweet_df.duplicated(['Tweets']).tolist()\n",
    "print(duplicate_tweets.count(True))\n",
    "\n",
    "#Remove duplicates from the dataframe.\n",
    "final_covidtweet_df = final_covidtweet_df.drop_duplicates(['Tweets'])\n",
    "final_covidtweet_df.shape[0]\n",
    "\n",
    "# Printing tweet text to spot the unwanted string or special characters from the text.\n",
    "print(final_covidtweet_df[\"Tweets\"])\n",
    "\n",
    "\n",
    "# Removing the unwanted string and special charcters using regex pattern.\n",
    "def CleaningTweetsText(reqexClean):\n",
    "    reqexClean = re.sub(r'@[A-Z0-9a-z_:]+','',reqexClean)\n",
    "    reqexClean = re.sub(r'^[RT]+','',reqexClean)\n",
    "    reqexClean = re.sub('https?://[A-Za-z0-9./]+','',reqexClean)\n",
    "    reqexClean = re.sub(\"[^a-zA-Z]\",' ',reqexClean)\n",
    "    return reqexClean.lower()\n",
    "\n",
    "#Calling the CleaningTweetsText function and storing the cleaned tweets in separtate column called CleanedTweetsText\n",
    "final_covidtweet_df['Cleaned_Tweets'] = final_covidtweet_df['Tweets'].apply(lambda x: CleaningTweetsText(x))\n",
    "\n",
    "#Printing the tweets text after removing unwanted string and special charcters\n",
    "print(final_covidtweet_df[\"Cleaned_Tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing NLP techniques during data pre-processing.\n",
    "for i in range(len(final_covidtweet_df.index)):\n",
    "        tweet_text = final_covidtweet_df.iloc[i]['Cleaned_Tweets']\n",
    "        word_tokens = nltk.word_tokenize(tweet_text)\n",
    "        stop_words_removal = [w for w in word_tokens if not w in stop_words]\n",
    "        word_pos_tag = pos_tag(word_tokens)\n",
    "        tag_words_var = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in word_pos_tag]\n",
    "        \n",
    "#Printing the results after tokenzied.\n",
    "print(word_tokens)\n",
    "\n",
    "#Printing the results after pos-tagging.\n",
    "print(tag_words_var)\n",
    "\n",
    "#Printing the results after Stop words removal.\n",
    "print( stop_words_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Calculating sentiment_score\n",
    "    def calculating_sentiment(final_covidtweet_sentiscore):\n",
    "    col_wd_swn=[]\n",
    "    col_pos_wd_swn=[]\n",
    "    col_neg_wd_swn=[]\n",
    "    mis_words=[]\n",
    "    for i in range(len(final_covidtweet_sentiscore.index)):\n",
    "        tweet_text = final_covidtweet_sentiscore.iloc[i]['Cleaned_Tweets']\n",
    "        word_tokens = nltk.word_tokenize(tweet_text)\n",
    "        tag_part_of_speech = pos_tag(word_tokens)\n",
    "        tagged_words = [(wd, map_tag('en-ptb', 'universal', t)) for wd, t in tag_part_of_speech]\n",
    "\n",
    "        total_positive_Score=0\n",
    "        total_negative_Score=0\n",
    "        for wd,tags in tagged_words:\n",
    "            if(tags=='NOUN'):\n",
    "                tags='n'\n",
    "            elif(tags=='VERB'):\n",
    "                tags='v'\n",
    "            elif(tags=='ADJ'):\n",
    "                tags='a'\n",
    "            elif(tags=='ADV'):\n",
    "                tags = 'r'\n",
    "            else:\n",
    "                tags='nothing'\n",
    "\n",
    "            if(tags!='nothing'):\n",
    "                join_tagged_words = wd+'.'+tags+'.01'\n",
    "                try:\n",
    "                    this_word_pos=swn.senti_synset(join_tagged_words).pos_score()\n",
    "                    this_word_neg=swn.senti_synset(join_tagged_words).neg_score()\n",
    "                except Exception as err:\n",
    "                    lem_word = lem.lemmatize(wd)\n",
    "                    join_tagged_words = lem_word+'.'+tags+'.01'\n",
    "                    try:\n",
    "                        this_word_pos=swn.senti_synset(join_tagged_words).pos_score()\n",
    "                        this_word_neg=swn.senti_synset(join_tagged_words).neg_score()\n",
    "                    except Exception as err:\n",
    "                        stem_word = pstem.stem(wd)\n",
    "                        join_tagged_words = stem_word+'.'+tags+'.01'\n",
    "                        try:\n",
    "                            this_word_pos=swn.senti_synset(join_tagged_words).pos_score()\n",
    "                            this_word_neg=swn.senti_synset(join_tagged_words).neg_score()\n",
    "                        except:\n",
    "                            mis_words.append(wd)\n",
    "                            continue\n",
    "                total_positive_Score+=this_word_pos\n",
    "                total_negative_Score+=this_word_neg\n",
    "        col_pos_wd_swn.append(total_positive_Score)\n",
    "        col_neg_wd_swn.append(total_negative_Score)\n",
    "\n",
    "        if(total_positive_Score!=0 or total_negative_Score!=0):\n",
    "            if(total_positive_Score>total_negative_Score):\n",
    "                col_wd_swn.append(1)\n",
    "            else:\n",
    "                col_wd_swn.append(-1)\n",
    "        else:\n",
    "            col_wd_swn.append(0)\n",
    "    final_covidtweet_sentiscore.insert(6,\"positive_metrics\",col_pos_wd_swn,True)\n",
    "    final_covidtweet_sentiscore.insert(7,\"negative_metrics\",col_neg_wd_swn,True)\n",
    "    final_covidtweet_sentiscore.insert(8,\"sentiment_metrics\",col_wd_swn,True)\n",
    "    return final_covidtweet_sentiscore\n",
    "\n",
    "#Creating a new dataframe called sentiment score\n",
    "sentiment_dataframe = pd.DataFrame()\n",
    "\n",
    "#Calling the caluclating sentiment function and storing the values into the dataframe\n",
    "sentiment_dataframe = calculating_sentiment(final_covidtweet_df)\n",
    "\n",
    "#Reomving the unwnated columns for research\n",
    "sentiment_dataframe = sentiment_dataframe.drop(['Unnamed: 0', 'S.NO', 'Tweets'], axis=1)\n",
    "\n",
    "#Printing length of the column.\n",
    "print(len(sentiment_dataframe.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the category values to text from numeric.\n",
    "def sentiment_category(x):\n",
    "    if x == -1:\n",
    "        return 'negative'\n",
    "    elif x == 1:\n",
    "        return 'positive'\n",
    "    elif x == 0 :\n",
    "        return 'neutral'\n",
    "\n",
    "# Calling the function called sentiment_category and storing the values into sentiment category column\n",
    "sentiment_dataframe['Sentiment_category'] = sentiment_dataframe['sentiment_metrics'].apply(lambda x: sentiment_category(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating WordCloud for positive, negative and neutral tweets.\n",
    "sentiment_dataframe['Tweet_without_stopwords'] = sentiment_dataframe['Cleaned_Tweets']\n",
    ".apply(lambda x: ' '.join([wd for wd in x.split() if wd not in (stop_words)]))\n",
    "\n",
    "#List of positive, negative and neutral tweets.\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "neutral_words = []\n",
    "\n",
    "#Storing each category of tweets in separate list.\n",
    "for x in range(len(sentiment_dataframe.index)):\n",
    "    if(sentiment_dataframe.iloc[x][\"Sentiment_category\"]=='positive'):\n",
    "        positive_words+=sentiment_dataframe.iloc[x][\"Tweet_without_stopwords\"]\n",
    "    elif(sentiment_dataframe.iloc[x][\"Sentiment_category\"]=='negative'):\n",
    "        negative_words+=sentiment_dataframe.iloc[x][\"Tweet_without_stopwords\"]\n",
    "    else:\n",
    "        neutral_words+=sentiment_dataframe.iloc[x][\"Tweet_without_stopwords\"]\n",
    "\n",
    "list_category_words = [positive_words,negative_words,neutral_words]\n",
    "\n",
    "# Word cloud visualization.\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "for category_words in list_category_words:\n",
    "    word_cloud = WordCloud(width = 600,height = 600,max_font_size = 200).generate(''.join(category_words))\n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(word_cloud,interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save file for PowerBI_visualization.\n",
    "filename = 'Tweet_sentiment_score_PowerBI_visualization.csv'\n",
    "sentiment_dataframe.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a model using LInearSVC and TF-IDFVectorizer\n",
    "SEED = 4\n",
    "x = sentiment_dataframe.Tweet_without_stopwords\n",
    "y = sentiment_dataframe.sentiment_metrics\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.1,random_state=SEED)\n",
    "\n",
    "#Coverting text data into numeric using TfidfVectorizer\n",
    "Tfidvec = TfidfVectorizer(decode_error='ignore',lowercase=False,max_features=11)\n",
    "x_train_copy = x_train\n",
    "x_traintdf = Tfidvec.fit_transform(x_train_copy)\n",
    "\n",
    "#Output after TfidfVectorizerss \n",
    "x_traintdf\n",
    "\n",
    "#SVM classifier model\n",
    "tdfvec = TfidfVectorizer(ngram_range=(1,2))\n",
    "model = make_pipeline(tdfvec, LinearSVC())\n",
    "model.fit(x_train,y_train)\n",
    "predicted_results = model.predict(x_test.values)\n",
    "accuracy_score = accuracy_score(y_test,predicted_results)\n",
    "precision_score = precision_score(y_test,predicted_results)\n",
    "recall_score = recall_score(y_test,predicted_results)\n",
    "print(2,\" gram\", \"accuracy_score: \" + str(accuracy_score), \"precision_score: \" + str(precision_score.mean()), \"recall_score: \" + str(recall_score.mean()))\n",
    "print('Predicted_vales:',labels)\n",
    "print('acutual_values:',y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using NRC lexicon to detect emotion from tweet.\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import csv\n",
    "dict_with_Word_emotion = defaultdict(list)\n",
    "dict_with_emotion_word = defaultdict(list)\n",
    "with open('D:\\\\Study\\\\Dissertation\\\\Practical\\\\NRC\\\\NRC-emotion-lexicon-wordlevel-alphabetized-v0.92.txt', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    headerRows = [i for i in range(0, 46)]\n",
    "    for row in headerRows:\n",
    "        next(reader)\n",
    "    for word, emotion, present in reader:\n",
    "        if int(present) == 1:\n",
    "            dict_with_Word_emotion[word].append(emotion)\n",
    "            dict_with_emotion_word[emotion].append(word)\n",
    "\n",
    "# Getting count of emotions from the text.\n",
    "def generate_emotion_count(string, tokenizer):\n",
    "    emoCount = Counter()\n",
    "    for token in tt.tokenize(string):\n",
    "        token = token.lower()\n",
    "        emoCount += Counter(dict_with_Word_emotion[token])\n",
    "    return emoCount\n",
    "\n",
    "# Collecting the cleaned tweets and tokenzing the tweets\n",
    "tweets = sentiment_dataframe['Cleaned_Tweets']\n",
    "tt = TweetTokenizer()\n",
    "\n",
    "# Calculating emotion count using NRC lexicon\n",
    "emotionCounts = [generate_emotion_count(twt, tt) for twt in tweets]\n",
    "\n",
    "# Removing Null values from emotion_df dataframe.\n",
    "emotion_df = pd.DataFrame(emotionCounts, index=tweets.index)\n",
    "emotion_df = emotion_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new columns in sentiment datframe which are in the emotion dataframe\n",
    "sentiment_dataframe['trust'] = emotion_df['trust']\n",
    "sentiment_dataframe['surprise'] = emotion_df['surprise']\n",
    "sentiment_dataframe['joy'] = emotion_df['joy']\n",
    "sentiment_dataframe['anger'] = emotion_df['anger']\n",
    "sentiment_dataframe['sadness'] = emotion_df['sadness']\n",
    "sentiment_dataframe['fear'] = emotion_df['fear']\n",
    "sentiment_dataframe['anticipation'] = emotion_df['anticipation']\n",
    "sentiment_dataframe['disgust'] = emotion_df['disgust']\n",
    "\n",
    "sentiment_dataframe.head(5)\n",
    "\n",
    "# Saving the data for powerBI visualization.\n",
    "filename = 'Emotion_tweet_count.csv'\n",
    "sentiment_dataframe.to_csv(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
