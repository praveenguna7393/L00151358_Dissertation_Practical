{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supporting packages for extracting tweets from Twitter API\n",
    "import tweepy as tw_access\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access tokens for Twitter API\n",
    "consumer_key = '7BHTvbd8lcEzHL0KrU2m16pXY'\n",
    "consumer_secret = 'zgWwCnyYnPY1TKbaYFJHjxfJFziwFh6KKnHMKJc0isZ9JJJD0V'\n",
    "access_token = '1233301007163105283-eWvFHQFbP4IOeD11eley5vDprQ3xJB'\n",
    "access_secret = 'LsCH91TO8ca8jyAMacSViFHAXGj5Y8JC3FJOkMttq6ptQ'\n",
    "\n",
    "#Calling twitter API using OAuthHandler\n",
    "twitter_auth = tw_access.OAuthHandler(consumer_key, consumer_secret)\n",
    "twitter_auth.set_access_token(access_token, access_secret)\n",
    "Twitter_API = tw_access.API(twitter_auth, wait_on_rate_limit = True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the sample results from twitter API\n",
    "search_for_hashtag = \"#coronavirus OR #COVID19 OR #COVID-19 OR #LOCKDOWN OR #flatternthecurve OR #quarantine OR #wuhanvirus OR #coronavirusoutbreak OR #pandemic OR #socialdistancing OR #SARSCoV2 OR #coronaviruspandemic OR #COVID19UK OR #UKlockdown OR #coronavirusUSA OR #COVID19US OR #TrumpLiesPeopleDie OR #COVIDIOTS OR #CoronaVirus OR #IdiotInCheif OR #LockdownUSA OR #coronavirusuk OR #COVIDIDIOTS\"\n",
    "data_since = \"2020-07-03\"\n",
    "\n",
    "tweets = tw.Cursor(api.search, q=search_for_hashtag, lang=\"en\",since=data_since).items(100)\n",
    "tweets\n",
    "\n",
    "tweet_details = [[tweet.geo, tweet.text, tweet.user.screen_name,tweet.user.location] for tweet in tweets]\n",
    "\n",
    "\n",
    "\n",
    "tweet_dataframe = pd.DataFrame(data=tweet_details, columns=['geo','text','user','location'])\n",
    "pd.set_option('max_colwidth', 800)\n",
    "tweet_dataframe.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to make a T times call because we can't go large number of tweets at one time.\n",
    "# Define for loop for T-Times to run a API to fetch the larger number of tweets.\n",
    "\n",
    "tweet_df = pd.DataFrame(columns = ['hashtags','text','user','location','totaltweets','retweetcount','acctdesc'])\n",
    "\n",
    "def gettweets(search_for_hashtag, data_since, numTweets, numRuns):\n",
    "\n",
    "    tweet_df = pd.DataFrame(columns = ['hashtags','text','user','location','totaltweets','retweetcount','acctdesc'])\n",
    "    program_start = time.time()\n",
    "    for i in range(0, numRuns):\n",
    "        start_run = time.time() # Time of tweet in single execution\n",
    "        \n",
    "        #Use cursor function to get the tweets.\n",
    "        tweets = tw.Cursor(api.search, q=search_for_hashtag, lang=\"en\", since=data_since).items(numTweets)\n",
    "        \n",
    "        #Collected tweets are stored in list.\n",
    "        Collection_tweets = [tw for tw in tweets]\n",
    "        noTweets = 0\n",
    "        \n",
    "        for tw in Collection_tweets:\n",
    "            un = tw.user.screen_name\n",
    "            acctdesc = tw.user.description\n",
    "            location = tw.user.location\n",
    "            retweetcount = tw.retweet_count\n",
    "            totaltweets = tw.user.statuses_count\n",
    "            hashtags = tw.entities['hashtags']\n",
    "            text = tw.text\n",
    "            \n",
    "            ith_tweet = [hashtags, text, username, location,totaltweets,retweetcount,acctdesc]\n",
    "            tweet_df.loc[len(tweet_df)] = ith_tweet\n",
    "            noTweets +=1\n",
    "            tweet_df.head(10)\n",
    "            \n",
    "        end_run = time.time()\n",
    "        duration_run= round((end_run-start_run)/60,2)\n",
    "        \n",
    "        count_data = tweet_dataframe.count()\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        print('count of the dataframe {} '.format(count_data))\n",
    "        \n",
    "        time.sleep(920)\n",
    "        \n",
    "        to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "        #path = os.getcwd()\n",
    "        filename = to_csv_timestamp + '_coronavirus_tweets.csv'\n",
    "        tweet_df.to_csv(filename)\n",
    "        \n",
    "        program_end = time.time()\n",
    "        print('Scraping has completed!')\n",
    "        print('Total time taken to srcap is {} minutes.'.format(round(program_end - program_start)/60,2))\n",
    "        \n",
    "# Calling get tweet function.\n",
    "search_for_hashtag = \"#coronavirus OR #COVID19 OR #COVID-19 OR #LOCKDOWN OR #flatternthecurve OR #quarantine OR #wuhanvirus OR #coronavirusoutbreak OR #pandemic OR #socialdistancing OR #SARSCoV2 OR #coronaviruspandemic OR #COVID19UK OR #UKlockdown OR #coronavirusUSA OR #COVID19US OR #TrumpLiesPeopleDie OR #COVIDIOTS OR #CoronaVirus OR #IdiotInCheif OR #LockdownUSA OR #coronavirusuk OR #COVIDIDIOTS\"\n",
    "data_since = \"2020-11-18\"\n",
    "numTweets = 2500\n",
    "numRuns = 6            \n",
    "gettweets(search_for_hashtag, data_since, numTweets, numRuns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
